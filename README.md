# Proyecto Final ‚Äì Operaciones de aprendizaje autom√°tico I  
### Implementaci√≥n en ambiente productivo de un modelo de ML para la Predicci√≥n de Arrestos en Cr√≠menes Reportados en la Ciudad de Chicago

---

## üë©‚Äçüíª Autores

- **Daniel Eduardo Pe√±aranda Peralta**
- **Jorge Adri√°n Alvarez**
- **Mar√≠a Bel√©n Cattaneo**  
- **Nicol√°s Valent√≠n Ciarrapico**  
- **Sabrina Daiana Pryszczuk**


Los servicios que contamos son:
- [Apache Airflow](https://airflow.apache.org/)
- [MLflow](https://mlflow.org/)
- API Rest para servir modelos ([FastAPI](https://fastapi.tiangolo.com/))
- [MinIO](https://min.io/)
- Base de datos relacional [PostgreSQL](https://www.postgresql.org/)
- Base de dato key-value [ValKey](https://valkey.io/) 

![Diagrama de servicios](final_assign.png)

Por defecto, cuando se inician los multi-contenedores, se crean los siguientes buckets:

- `s3://data`
- `s3://mlflow` (usada por MLflow para guardar los artefactos).

y las siguientes bases de datos:

- `mlflow_db` (usada por MLflow).
- `airflow` (usada por Airflow).

## Instalaci√≥n

1. Para poder levantar todos los servicios, primero instala [Docker](https://docs.docker.com/engine/install/) en tu computadora (o en el servidor que desees usar).
2. Clona este repositorio.
3. Crea las carpetas `airflow/config`, `airflow/dags`, `airflow/logs`, `airflow/plugins`.
4. Si est√°s en Linux o MacOS, en el archivo `.env`, reemplaza `AIRFLOW_UID` por el de tu usuario o alguno que consideres oportuno (para encontrar el UID, usa el comando `id -u <username>`). De lo contrario, Airflow dejar√° sus carpetas internas como root y no podr√°s subir DAGs (en `airflow/dags`) o plugins, etc.
5. En la carpeta ra√≠z de este repositorio, ejecuta:

```bash
make install && make up
```

O usando docker-compose directamente:

```bash
docker compose --profile all up
```

6. Una vez que todos los servicios est√©n funcionando (verifica con el comando `docker ps -a` que todos los servicios est√©n healthy o revisa en Docker Desktop), podr√°s acceder a los diferentes servicios mediante:
   - Apache Airflow: http://localhost:8080
   - MLflow: http://localhost:5001
   - MinIO: http://localhost:9001 (ventana de administraci√≥n de Buckets)
   - API: http://localhost:8800/
   - Documentaci√≥n de la API: http://localhost:8800/docs

Si est√°s usando un servidor externo a tu computadora de trabajo, reemplaza `localhost` por su IP (puede ser una privada si tu servidor est√° en tu LAN o una IP p√∫blica si no; revisa firewalls u otras reglas que eviten las conexiones).

Todos los puertos u otras configuraciones se pueden modificar en el archivo `.env`. Se invita a jugar y romper para aprender; siempre puedes volver a clonar este repositorio.

## Comandos disponibles (Makefile)

El proyecto incluye un Makefile con comandos √∫tiles para gestionar los servicios:

```bash
make help      # Muestra todos los comandos disponibles
make up        # Inicia todos los servicios
make down      # Detiene todos los servicios
make restart   # Reinicia todos los servicios
make install   # Reconstruye contenedores con nuevas dependencias
make clean     # Detiene y elimina todo (contenedores, redes y vol√∫menes)
make logs      # Muestra logs de todos los servicios en tiempo real
make status    # Muestra el estado de todos los servicios
```

### Flujo de trabajo com√∫n

**Primera vez:**
```bash
make install && make up
```

**Despu√©s de agregar dependencias en `requirements.txt`:**
```bash
make install
```

**Reiniciar desde cero:**
```bash
make clean
make install && make up
```

## ETL Pipeline: Chicago Crime Data

El proyecto incluye un pipeline ETL completo para an√°lisis de cr√≠menes en Chicago:

### Arquitectura del Pipeline

**Task 1: setup_s3**
- Crea bucket MinIO si no existe
- Configura pol√≠tica de lifecycle (TTL de 60 d√≠as para datos temporales)

**Task 2: download_data**
- Descarga datos de cr√≠menes desde Socrata API (Chicago Data Portal)
- Descarga datos de estaciones policiales
- Primera ejecuci√≥n: descarga a√±o completo (~260k registros)
- Ejecuciones subsecuentes: descarga incremental mensual (~25k registros)
- Guarda en MinIO: `raw-data/crimes_YYYY-MM-DD.csv` y `raw-data/police_stations.csv`

**Task 3: enrich_data**
- Carga datos desde MinIO
- Calcula distancia a estaci√≥n policial m√°s cercana (GeoPandas spatial join)
- Crea features temporales:
  - Season (Winter/Spring/Summer/Fall)
  - Day of week (0-6)
  - Day time (Morning/Afternoon/Evening/Night)
- Guarda en MinIO: `raw-data/crimes_enriched_YYYY-MM-DD.csv`

**Tasks 4-8:** (Por implementar)
- `split_data` - Divisi√≥n train/test (80/20 estratificado)
- `process_outliers` - Manejo de outliers y transformaci√≥n logar√≠tmica
- `encode_data` - Encoding de variables categ√≥ricas
- `scale_data` - Escalado con StandardScaler
- `balance_data` - Balanceo con SMOTE + RandomUnderSampler
- `extract_features` - Selecci√≥n de features con Mutual Information

### Estructura de M√≥dulos

```
airflow/dags/
‚îú‚îÄ‚îÄ etl_process_taskflow.py       # DAG principal con TaskFlow API
‚îî‚îÄ‚îÄ etl_helpers/
    ‚îú‚îÄ‚îÄ __init__.py               # Package initialization
    ‚îú‚îÄ‚îÄ minio_utils.py            # Operaciones MinIO/S3
    ‚îú‚îÄ‚îÄ data_loader.py            # Descarga desde Socrata API
    ‚îî‚îÄ‚îÄ data_enrichment.py        # Enriquecimiento geoespacial y temporal
```

### Configuraci√≥n

**Variables de entorno requeridas (`.env`):**
```bash
SOCRATA_APP_TOKEN=tu_token_aqui  # Token de Socrata API
DATA_REPO_BUCKET_NAME=data        # Bucket MinIO para datos
```

**Dependencias principales:**
- `sodapy` - Cliente Socrata API
- `geopandas` - C√°lculos geoespaciales
- `shapely` - Geometr√≠as
- `pandas` - Manipulaci√≥n de datos

### Ejecuci√≥n

**Trigger manual:**
1. Abrir Airflow UI: `make airflow`
2. Localizar DAG: `etl_with_taskflow`
3. Click en "Play" para ejecutar

**Ejecuci√≥n autom√°tica:**
- Schedule: `@monthly` (primer d√≠a de cada mes)
- Primera ejecuci√≥n: descarga a√±o completo
- Subsecuentes: solo √∫ltimo mes

### Monitoreo

**Ver logs:**
```bash
make logs                    # Todos los servicios
docker logs -f <container>   # Servicio espec√≠fico
```

**Ver estado:**
```bash
make status
```

**Acceder a UIs:**
```bash
make airflow   # Ver DAG runs y logs
make minio     # Ver archivos en buckets
```

### Datos de Salida

**Ubicaci√≥n:** MinIO bucket `data/`

**Estructura:**
```
data/
‚îú‚îÄ‚îÄ 0-raw-data/
‚îÇ   ‚îú‚îÄ‚îÄ monthly-data/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ {YYYY-MM}/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ crimes.csv              # Cr√≠menes descargados del mes
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ police_stations.csv     # Estaciones policiales
‚îÇ   ‚îî‚îÄ‚îÄ data/
‚îÇ       ‚îî‚îÄ‚îÄ crimes_12m_{YYYY-MM-DD}.csv # Ventana rolling 12 meses
‚îú‚îÄ‚îÄ 1-enriched-data/
‚îÇ   ‚îî‚îÄ‚îÄ crimes_enriched_{YYYY-MM-DD}.csv # Cr√≠menes enriquecidos
‚îî‚îÄ‚îÄ 2-processed-data/                    # (Pr√≥ximos pasos)
    ‚îú‚îÄ‚îÄ train_encoded.csv
    ‚îú‚îÄ‚îÄ test_encoded.csv
    ‚îî‚îÄ‚îÄ ...
```

## Apagar los servicios

Estos servicios ocupan cierta cantidad de memoria RAM y procesamiento, por lo que cuando no se est√°n utilizando, se recomienda detenerlos. Para hacerlo, ejecuta:

```bash
make down
```

O usando docker-compose directamente:

```bash
docker compose --profile all down
```

Si deseas no solo detenerlos, sino tambi√©n eliminar toda la infraestructura (liberando espacio en disco):

```bash
make clean
```

O usando docker-compose directamente:

```bash
docker compose down --rmi all --volumes
```

**Nota:** Si haces esto, perder√°s todo en los buckets y bases de datos.

## Aspectos espec√≠ficos de Airflow

### Variables de entorno
Airflow ofrece una amplia gama de opciones de configuraci√≥n. En el archivo `docker-compose.yaml`, dentro de `x-airflow-common`, se encuentran variables de entorno que pueden modificarse para ajustar la configuraci√≥n de Airflow. Pueden a√±adirse [otras variables](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html).

### Uso de ejecutores externos
Actualmente, para este caso, Airflow utiliza un ejecutor [celery](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/executor/celery.html), lo que significa que las tareas se ejecutan en otro contenedor. 

### Uso de la CLI de Airflow

Si necesitan depurar Apache Airflow, pueden utilizar la CLI de Apache Airflow de la siguiente manera:

```bash
docker compose --profile all --profile debug up
```

Una vez que el contenedor est√© en funcionamiento, pueden utilizar la CLI de Airflow de la siguiente manera, 
por ejemplo, para ver la configuraci√≥n:

```bash
docker-compose run airflow-cli config list      
```

Para obtener m√°s informaci√≥n sobre el comando, pueden consultar [aqui](https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html).

### Variables y Conexiones

Si desean agregar variables para accederlas en los DAGs, pueden hacerlo en `secrets/variables.yaml`. Para obtener m√°s [informaci√≥n](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/variables.html), 
consulten la documentaci√≥n.

Si desean agregar conexiones en Airflow, pueden hacerlo en `secrets/connections.yaml`. Tambi√©n es posible agregarlas mediante la interfaz de usuario (UI), pero estas no persistir√°n si se borra todo. Por otro lado, cualquier conexi√≥n guardada en `secrets/connections.yaml` no aparecer√° en la UI, aunque eso no significa que no exista. Consulten la documentaci√≥n para obtener m√°s 
[informaci√≥n](https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/connections.html).

## Conexi√≥n con los buckets

Dado que no estamos utilizando Amazon S3, sino una implementaci√≥n local de los mismos mediante MinIO, es necesario modificar las variables de entorno para conectar con el servicio de MinIO. Las variables de entorno son las siguientes:

```bash
AWS_ACCESS_KEY_ID=minio   
AWS_SECRET_ACCESS_KEY=minio123 
AWS_ENDPOINT_URL_S3=http://localhost:90000
```

MLflow tambi√©n tiene una variable de entorno que afecta su conexi√≥n a los buckets:

```bash
MLFLOW_S3_ENDPOINT_URL=http://localhost:9000
```
Aseg√∫rate de establecer estas variables de entorno antes de ejecutar tu notebook o scripts en tu m√°quina o en cualquier otro lugar. Si est√°s utilizando un servidor externo a tu computadora de trabajo, reemplaza localhost por su direcci√≥n IP.

Al hacer esto, podr√°s utilizar `boto3`, `awswrangler`, etc., en Python con estos buckets, o `awscli` en la consola.

Si tienes acceso a AWS S3, ten mucho cuidado de no reemplazar tus credenciales de AWS. Si usas las variables de entorno, no tendr√°s problemas.

## Valkey

La base de datos Valkey es usada por Apache Airflow para su funcionamiento. Tal como est√° configurado ahora no esta expuesto el puerto para poder ser usado externamente. Se puede modificar el archivo `docker-compose.yaml` para habilitaro.
