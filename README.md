# Chicago Crime Arrest Prediction - MLOps Pipeline

[![Python 3.12](https://img.shields.io/badge/python-3.12-blue.svg)](https://www.python.org/downloads/)
[![Airflow 2.10](https://img.shields.io/badge/airflow-2.10-green.svg)](https://airflow.apache.org/)
[![MLflow 2.10](https://img.shields.io/badge/mlflow-2.10-orange.svg)](https://mlflow.org/)
[![FastAPI](https://img.shields.io/badge/fastapi-0.115-teal.svg)](https://fastapi.tiangolo.com/)

End-to-end MLOps pipeline for predicting arrest probability in Chicago crime incidents.

## ğŸ‘¥ Authors

- Daniel Eduardo PeÃ±aranda Peralta
- Jorge AdriÃ¡n Alvarez
- MarÃ­a BelÃ©n Cattaneo
- NicolÃ¡s ValentÃ­n Ciarrapico
- Sabrina Daiana Pryszczuk

---

## ğŸ“‹ Table of Contents

- [Architecture](#-architecture)
- [Quick Start](#-quick-start)
- [Services](#-services)
- [ETL Pipeline](#-etl-pipeline)
- [Model Training](#-model-training)
- [API Reference](#-api-reference)
- [Configuration](#-configuration)
- [Troubleshooting](#-troubleshooting)

---

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           DATA SOURCES                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚ Chicago Data Portal â”‚    â”‚   Police Stations   â”‚                     â”‚
â”‚  â”‚   (Socrata API)     â”‚    â”‚      (Static)       â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                          â”‚
              â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ORCHESTRATION (Airflow)                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚Downloadâ”‚â†’â”‚ Merge  â”‚â†’â”‚Enrich  â”‚â†’â”‚ Split  â”‚â†’â”‚Outliersâ”‚â†’â”‚ Encode â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                              â”‚           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                           â–¼           â”‚
â”‚  â”‚Summary â”‚â†â”‚Featuresâ”‚â†â”‚Balance â”‚â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         STORAGE (MinIO S3)                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ s3://data/ml-ready-data/train_YYYY-MM-DD.csv                    â”‚    â”‚
â”‚  â”‚ s3://data/ml-ready-data/test_YYYY-MM-DD.csv                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EXPERIMENT TRACKING (MLflow)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚   XGBoost       â”‚  â”‚ Random Forest   â”‚  â”‚    Other...     â”‚         â”‚
â”‚  â”‚   MCC: 0.54     â”‚  â”‚   MCC: 0.52     â”‚  â”‚                 â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚           â”‚                                                              â”‚
â”‚           â–¼                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              MODEL REGISTRY (champion alias)                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         SERVING (FastAPI)                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  POST /predict       - Single prediction                        â”‚    â”‚
â”‚  â”‚  POST /predict/batch - Batch predictions (up to 1000)           â”‚    â”‚
â”‚  â”‚  GET  /model/info    - Model metadata and metrics               â”‚    â”‚
â”‚  â”‚  GET  /health        - Health check (200/503)                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisites

- [Docker](https://docs.docker.com/engine/install/) (with Docker Compose)
- 8GB RAM minimum
- 10GB disk space

### Installation

```bash
# 1. Clone repository
git clone <repository-url>
cd MLOPS-main

# 2. Create directories
mkdir -p airflow/{config,dags,logs,plugins}
chmod 777 airflow/logs

# 3. Configure environment
cat > .env << EOF
AIRFLOW_UID=$(id -u)
SOCRATA_APP_TOKEN=your_token_here
DATA_REPO_BUCKET_NAME=data
EOF

# 4. Start services
make install && make up

# 5. Verify (all services should be "healthy")
docker ps
```

### Get Socrata API Token

1. Go to [Chicago Data Portal](https://data.cityofchicago.org/)
2. Sign in â†’ My Profile â†’ Developer Settings
3. Create New App Token
4. Copy token to `.env` file

### Access Services

| Service | URL | Credentials |
|---------|-----|-------------|
| **Airflow** | http://localhost:8080 | `airflow` / `airflow` |
| **MLflow** | http://localhost:5001 | - |
| **MinIO** | http://localhost:9001 | `minio` / `minio123` |
| **API Docs** | http://localhost:8800/docs | - |

---

## ğŸ”§ Services

| Service | Purpose | Port |
|---------|---------|------|
| **Airflow** | ETL orchestration | 8080 |
| **MLflow** | Experiment tracking & model registry | 5001 |
| **MinIO** | S3-compatible object storage | 9000/9001 |
| **PostgreSQL** | Metadata storage | 5432 |
| **FastAPI** | Model serving REST API | 8800 |
| **Valkey** | Celery broker (Redis fork) | 6379 |

---

## ğŸ“Š ETL Pipeline

The `etl_with_taskflow` DAG processes Chicago crime data through 11 stages:

| Stage | Description | Output |
|-------|-------------|--------|
| 1. Setup S3 | Create bucket, set lifecycle | - |
| 2. Download | Fetch from Socrata API | `0-raw-data/` |
| 3. Merge | Rolling 12-month window | `1-merged-data/` |
| 4. Enrich | Add geospatial + temporal features | `2-enriched-data/` |
| 5. Split | Train/test (80/20 stratified) | `3-split-data/` |
| 6. Outliers | Remove outliers (Â±3Ïƒ) | `4-outliers/` |
| 7. Encode | Frequency, cyclic, one-hot encoding | `5-encoded/` |
| 8. Scale | StandardScaler normalization | `6-scaled/` |
| 9. Balance | SMOTE + RandomUnderSampler | `7-balanced/` |
| 10. Features | Mutual Information selection | `ml-ready-data/` |
| 11. Summary | Log pipeline metrics to MLflow | - |

### Run ETL

**Manual trigger:**
```bash
# Via Airflow UI
open http://localhost:8080  # DAG: etl_with_taskflow â†’ Play

# Via CLI
docker-compose run airflow-cli dags trigger etl_with_taskflow
```

**Schedule:** Monthly (1st day at 00:00)

---

## ğŸ§ª Model Training

### Train XGBoost Model

```bash
docker-compose run trainer python mlflow_xgboost_poc_docker.py
```

### Assign Champion Alias

    %% --- 1. ETL ---
    subgraph S1 ["1. ETL PIPELINE (Airflow)"]
        direction TB
        %% Definimos el flujo interno
        N1("ğŸ“¥ Download Data &rarr; Enrich Data<br>âš™ï¸ Process Data&rarr; ML-Ready Data"):::title
        D1("ğŸ’¾ MinIO s3://data/<br>------------------<br>â€¢ [Raw Data]<br>â€¢ [Enriched Data]<br>â€¢ [Processed]<br>â€¢ [Train/Test]"):::content
        N1 --> D1
    end

    %% --- 2. EXPERIMENTACIÃ“N ---
    subgraph S2 ["2. EXPERIMENTACIÃ“N"]
        direction TB
        N2("ğŸ§ª Notebooks/Scripts"):::title
        D2("ğŸ¤– Modelos:<br>â€¢ Exp. 1: Logistic Regression<br>â€¢ Exp. 2: Random Forest<br>â€¢ Exp. 3: XGBoost<br><br>ğŸ“Š MLflow Tracking UI:<br>â€¢ Metrics: Accuracy, Precision, Recall, F1, AUC<br>â€¢ Params: Hyperparameters, Features, Data version<br>â€¢ Artifacts: Model, Charts, Feature importance"):::content
        N2 --> D2
    end

    %% --- 3. REGISTRO DE MODELO---
    subgraph S3 ["3. REGISTRO (MLflow)"]
        direction TB
        N3("ğŸ¥‡ Model Registry"):::title
        D3("ğŸ“‹ Flujo:<br>1. Seleccionar Mejor<br>2. Registrar VersiÃ³n<br>3. Stage: 'Staging'<br>4. Stage: 'Production'"):::content
        N3 --> D3
    end

    %% --- 4. DESPLIEGUE ---
    subgraph S4 ["4. DESPLIEGUE (FastAPI)"]
        direction TB
        N4("ğŸš€ API REST"):::title
        D4("ğŸ”Œ Endpoints:<br>------------------<br>â€¢ POST /predict<br>  (PredicciÃ³n individual)<br>â€¢ POST /predict/batch<br>  (PredicciÃ³n por lote)<br>â€¢ GET /model/info (Info del modelo en uso)"):::content
        N4 --> D4
    end

    %% --- 5. MONITOREO ---
    subgraph S5 ["5. MANTENIMIENTO"]
        direction TB
        N5("ğŸ›¡ï¸ Monitoreo"):::title
        D5("âš ï¸ Data Drift check<br>ğŸ“ˆ Performance tracking<br>ğŸ”„ Automated Retraining (AirFlow DAG)<br>ğŸ†š A/B Testing (Champion/Challenger)"):::content
        N5 --> D5
    end

    %% --- CONEXIONES ---
    S1 --> S2
    S2 --> S3
    S3 --> S4
    S4 --> S5
    
    %% --- RETROALIMENTACION ---
    D5 -.-> |Trigger New Run| S1
```
---

## ğŸ”§ ETL Pipeline

### DescripciÃ³n General

Pipeline ETL automatizado que procesa datos de crÃ­menes de Chicago desde la API pÃºblica hasta datasets ML-ready.

### Arquitectura del Pipeline
<details>
  <summary><strong>ğŸ”» Clic aquÃ­ para ver el Diagrama de Flujo VersiÃ³n Texto (ASCII)</strong></summary>

  ```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Setup     â”‚ â†’ â”‚  Download   â”‚ â†’ â”‚   Enrich    â”‚ â†’ â”‚    Split    â”‚
â”‚     S3      â”‚   â”‚    Data     â”‚   â”‚    Data     â”‚   â”‚    Data     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Extract   â”‚ â† â”‚   Balance   â”‚ â† â”‚    Scale    â”‚ â† â”‚   Encode    â”‚
â”‚  Features   â”‚   â”‚    Data     â”‚   â”‚    Data     â”‚   â”‚    Data     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Pipeline  â”‚
â”‚   Summary   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
</details>

```mermaid
graph LR
    %% Cambiamos a LR (Left to Right) para que sea horizontal
    
    %% Estilos "Dark Mode Friendly" (Fondo oscuro, letra blanca)
    classDef steps fill:#1f425f,stroke:#82b1ff,stroke-width:2px,color:white,rx:5,ry:5;
    classDef startend fill:#1b4d3e,stroke:#4cc9f0,stroke-width:2px,color:white,rx:5,ry:5;

    %% Nodos
    A[Setup<br>S3]:::startend
    B[Download<br>Data]:::steps
    C[Enrich<br>Data]:::steps
    D[Split<br>Data]:::steps
    E[Encode<br>Data]:::steps
    F[Scale<br>Data]:::steps
    G[Balance<br>Data]:::steps
    H[Extract<br>Features]:::steps
    I[Pipeline<br>Summary]:::startend

    %% Conexiones
    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    F --> G
    G --> H
    H --> I
```

### Etapas del Pipeline

#### 1ï¸âƒ£ Setup S3
- Crea bucket MinIO si no existe
- Configura polÃ­tica de lifecycle (TTL 60 dÃ­as para datos temporales)

#### 2ï¸âƒ£ Download Data
- **Fuente:** [Chicago Data Portal](https://data.cityofchicago.org/) via Socrata API
- **Descarga inicial:** AÃ±o completo (~450k registros)
- **Descargas subsecuentes:** Incremental mensual (~35k registros)
- **Output:** `s3://data/0-raw-data/monthly-data/{YYYY-MM}/crimes.csv`

#### 3ï¸âƒ£ Enrich Data
- **Geoespacial:** Distancia a estaciÃ³n policial mÃ¡s cercana (GeoPandas)
- **Temporal:**
  - Season (Winter/Spring/Summer/Autumn)
  - Day of week (0=Monday, 6=Sunday) - encoded as sine for cyclical pattern
  - Day time (Early Morning/Morning/Afternoon/Night)
- **Limpieza:** Duplicados, valores nulos
- **Output:** `s3://data/1-enriched-data/crimes_enriched_{date}.csv`
- **Monitoring:** Logs raw data quality metrics to MLflow

#### 4ï¸âƒ£ Split Data
- **Estrategia:** Stratified train/test split (80/20)
- **Target:** `arrest` (boolean)
- **Output:**
  - `s3://data/2-split-data/crimes_train_{date}.csv`
  - `s3://data/2-split-data/crimes_test_{date}.csv`
- **Monitoring:** Logs class distribution to MLflow

#### 5ï¸âƒ£ Process Outliers
- **MÃ©todo:** IQR-based outlier removal
- **Log transformation:** `distance_crime_to_police_station`
- **Output:** `s3://data/3-outliers-data/`

#### 6ï¸âƒ£ Encode Data
- **One-hot:** Low cardinality (season, day_time)
- **Frequency:** High cardinality (primary_type, location_description)
- **Cyclic:** Day of week (sine transformation)
- **Label:** Boolean features (domestic)
- **Output:** `s3://data/4-encoded-data/`

#### 7ï¸âƒ£ Scale Data
- **MÃ©todo:** StandardScaler (zero mean, unit variance)
- **Features:** Numeric only (coordinates, distances)
- **Output:** `s3://data/5-scaled-data/`

#### 8ï¸âƒ£ Balance Data
- **Problema:** ~84% no arrest, ~16% arrest
- **SoluciÃ³n:** SMOTE + RandomUnderSampler
  - SMOTE: Oversample minority to 50% of majority
  - Undersampling: Final ratio 80% (minority = 80% of majority)
- **Output:** `s3://data/6-balanced-data/`
- **Monitoring:** Logs balancing impact to MLflow

#### 9ï¸âƒ£ Extract Features
- **MÃ©todo:** Mutual Information feature selection
- **Threshold:** MI score > 0.05
- **Features finales:** ~11 features (de ~20 originales)
- **Output:** `s3://data/ml-ready-data/train_{date}.csv`
- **Monitoring:** Logs feature importance and correlation to MLflow

#### ğŸ”Ÿ Pipeline Summary
- **ConsolidaciÃ³n:** MÃ©tricas de todo el pipeline
- **VisualizaciÃ³n:** Flow chart mostrando transformaciÃ³n de datos
- **Output:** MLflow run con pipeline overview

### Estructura de Datos en MinIO

```
s3://data/
â”œâ”€â”€ 0-raw-data/
â”‚   â””â”€â”€ monthly-data/
â”‚       â””â”€â”€ {YYYY-MM}/
â”‚           â”œâ”€â”€ crimes.csv              # ~35k registros/mes
â”‚           â””â”€â”€ police_stations.csv     # 23 estaciones
â”œâ”€â”€ 1-enriched-data/
â”‚   â””â”€â”€ crimes_enriched_{date}.csv      # +3 features temporales
â”œâ”€â”€ 2-split-data/
â”‚   â”œâ”€â”€ crimes_train_{date}.csv         # 80%
â”‚   â””â”€â”€ crimes_test_{date}.csv          # 20%
â”œâ”€â”€ 3-outliers-data/
â”‚   â”œâ”€â”€ train_{date}.csv
â”‚   â””â”€â”€ test_{date}.csv
â”œâ”€â”€ 4-encoded-data/
â”‚   â”œâ”€â”€ train_{date}.csv
â”‚   â””â”€â”€ test_{date}.csv
â”œâ”€â”€ 5-scaled-data/
â”‚   â”œâ”€â”€ train_{date}.csv
â”‚   â””â”€â”€ test_{date}.csv
â”œâ”€â”€ 6-balanced-data/
â”‚   â”œâ”€â”€ train_{date}.csv                # ~173k registros (balanced)
â”‚   â””â”€â”€ test_{date}.csv
â””â”€â”€ ml-ready-data/                      # â­ USAR ESTE PARA EXPERIMENTS
    â”œâ”€â”€ train_{date}.csv                # ~173k Ã— 11 features
    â””â”€â”€ test_{date}.csv                 # ~46k Ã— 11 features
```

### EjecuciÃ³n del Pipeline

**Trigger manual en Airflow UI:**
1. Navegar a http://localhost:8080
2. Buscar DAG: `etl_with_taskflow`
3. Click en â–¶ï¸ (Play) para ejecutar

**Schedule automÃ¡tico:**
- **Frecuencia:** `@monthly` (primer dÃ­a de cada mes a las 00:00)
- **Catchup:** Habilitado (procesa meses faltantes)
- **Max Active Runs:** 1 (evita ejecuciones concurrentes)

## Variables de entorno requeridas

Antes de ejecutar `make install`, asegÃºrate de configurar las siguientes variables en el archivo `.env`:

```bash
# Requeridas para el pipeline ETL
SOCRATA_APP_TOKEN=tu_token_aqui    # Token de Socrata API (ver instrucciones abajo)
DATA_REPO_BUCKET_NAME=data          # Bucket MinIO para datos

# Ya configuradas por defecto (modificar solo si es necesario)
AIRFLOW_UID=1000                    # UID del usuario para Airflow
AWS_ACCESS_KEY_ID=minio             # Credenciales MinIO
AWS_SECRET_ACCESS_KEY=minio123      # Credenciales MinIO
```

### Obtener Token de Socrata (Chicago Data Portal)

El pipeline ETL descarga datos del Chicago Data Portal usando la API de Socrata. Para evitar lÃ­mites de tasa, necesitas un App Token:

1. Ir a https://data.cityofchicago.org/
2. Crear una cuenta o iniciar sesiÃ³n (click en "Sign In" arriba a la derecha)
3. Una vez logueado, ir a tu perfil (click en tu nombre) â†’ "Developer Settings"
4. Click en "Create New App Token"
5. Completar el formulario:
   - **Application Name**: Nombre descriptivo (ej: "MLOps CEIA")
   - **Description**: DescripciÃ³n breve
   - **Website** (opcional): Puede dejarse vacÃ­o
6. Click en "Save" y copiar el **App Token** generado
7. Agregar el token al archivo `.env`:
   ```bash
   SOCRATA_APP_TOKEN=tu_token_generado_aqui
   ```

> **Nota**: Sin el token, el pipeline funcionarÃ¡ pero con lÃ­mites de velocidad mÃ¡s restrictivos.

## Comandos disponibles (Makefile)
### Monitoreo del Pipeline

Cada etapa del pipeline registra mÃ©tricas en **MLflow**:

**Runs creados automÃ¡ticamente:**
- `raw_data_{date}` - Calidad de datos crudos
- `split_{date}` - DistribuciÃ³n train/test
- `balance_{date}` - Impacto del balanceo
- `features_{date}` - Feature selection results
- `pipeline_summary_{date}` - Overview completo

**Artifacts en MLflow:**
- `charts/raw_data_overview.png` - 4 grÃ¡ficos de datos crudos
- `charts/split_distribution.png` - ComparaciÃ³n train/test
- `charts/balance_comparison.png` - Antes/despuÃ©s balanceo
- `charts/feature_importance.png` - Top 10 features (MI score)
- `charts/correlation_heatmap.png` - CorrelaciÃ³n entre features
- `charts/pipeline_flow.png` - â­ Data flow completo

---

## ğŸ§ª ExperimentaciÃ³n y Entrenamiento

### Acceso a Datos ML-Ready

Los datos procesados estÃ¡n disponibles en MinIO para tus experimentos:

```python
import os
import pandas as pd
import boto3

# Configurar conexiÃ³n a MinIO
os.environ["AWS_ACCESS_KEY_ID"] = "minio"
os.environ["AWS_SECRET_ACCESS_KEY"] = "minio123"
os.environ["AWS_ENDPOINT_URL_S3"] = "http://localhost:9000"

# Descargar datos
s3 = boto3.client('s3', endpoint_url='http://localhost:9000')

# Usar la Ãºltima versiÃ³n disponible (o especificar fecha)
train_df = pd.read_csv('s3://data/ml-ready-data/train_2025-11-22.csv')
test_df = pd.read_csv('s3://data/ml-ready-data/test_2025-11-22.csv')

# Separar features y target
X_train = train_df.drop('arrest', axis=1)
y_train = train_df['arrest']
X_test = test_df.drop('arrest', axis=1)
y_test = test_df['arrest']
```

### Template de ExperimentaciÃ³n

Ejemplo de experimento con tracking en MLflow:

```bash
docker-compose run trainer python -c "
import mlflow
mlflow.set_tracking_uri('http://mlflow:5000')
client = mlflow.tracking.MlflowClient()
client.set_registered_model_alias('xgboost_chicago_crimes', 'champion', '1')
print('Champion alias set!')
"
```

### View Experiments

Open MLflow UI: http://localhost:5001

---

## ğŸŒ API Reference

### Health Check

```bash
curl http://localhost:8800/health
# 200: {"status": "healthy", "model_loaded": true, ...}
# 503: {"status": "degraded", "model_loaded": false, ...}
```

### Single Prediction

```bash
curl -X POST http://localhost:8800/predict \
  -H "Content-Type: application/json" \
  -d '{
    "iucr_freq": 0.435,
  "primary_type_freq": 0.123,
    "location_description_freq": 0.045,
    
    "day_of_week_sin": 0.781,
    "x_coordinate_standardized": 1.234,
    
    "y_coordinate_standardized": -1.123,
    "distance_crime_to_police_station_standardized": 0.345
  }'
```

**Response:**
```json
{
  "prediction": true,
  "probability": 0.87,
  "model_version": "1",
  "timestamp": "2025-12-13T15:30:00Z"
}
```

### Batch Prediction

```bash
curl -X POST http://localhost:8800/predict/batch \
  -H "Content-Type: application/json" \
  -d '{
    "iucr_freq": 0.435,
    "primary_type_freq": 0.123,
    "location_description_freq": 0.045,
    "day_of_week_sin": 0.781,
    "x_coordinate_standardized": 1.234,
    "y_coordinate_standardized": -1.123,
    "distance_crime_to_police_station_standardized": 0.345
  }'
```

**Python:**
```python
import requests

url = "http://localhost:8800/predict"
data = {
    "primary_type_freq": 0.123,
    "location_description_freq": 0.045,
    # ... resto de features
}

response = requests.post(url, json=data)
print(response.json())
```

### Model Info

```bash
curl http://localhost:8800/model/info
```

### Reload Model

```bash
curl -X POST http://localhost:8800/model/reload
```

---

## âš™ Configuration

### Environment Variables

Create `.env` in project root:

```bash
# 1. Editar requirements.txt en dockerfiles/airflow/ o dockerfiles/fastapi/
# 2. Reconstruir
make install
```

**Reiniciar desde cero:**
```bash
make clean
make install && make up
```

## ETL Pipeline: Chicago Crime Data

El proyecto incluye un pipeline ETL completo para anÃ¡lisis de crÃ­menes en Chicago:

### Arquitectura del Pipeline

**Task 1: setup_s3**
- Crea bucket MinIO si no existe
- Configura polÃ­tica de lifecycle (TTL de 60 dÃ­as para datos temporales)

**Task 2: download_data**
- Descarga datos de crÃ­menes desde Socrata API (Chicago Data Portal)
- Descarga datos de estaciones policiales
- Primera ejecuciÃ³n: descarga aÃ±o completo (~260k registros)
- Ejecuciones subsecuentes: descarga incremental mensual (~25k registros)
- Guarda en MinIO: `raw-data/crimes_YYYY-MM-DD.csv` y `raw-data/police_stations.csv`

**Task 3: enrich_data**
- Carga datos desde MinIO
- Calcula distancia a estaciÃ³n policial mÃ¡s cercana (GeoPandas spatial join)
- Crea features temporales:
  - Season (Winter/Spring/Summer/Autumn)
  - Day of week (0-6) - encoded as sine for cyclical pattern
  - Day time (Early Morning/Morning/Afternoon/Night)
- Guarda en MinIO: `raw-data/crimes_enriched_YYYY-MM-DD.csv`

**Tasks 4-8:** (Por implementar)
- `split_data` - DivisiÃ³n train/test (80/20 estratificado)
- `process_outliers` - Manejo de outliers y transformaciÃ³n logarÃ­tmica
- `encode_data` - Encoding de variables categÃ³ricas
- `scale_data` - Escalado con StandardScaler
- `balance_data` - Balanceo con SMOTE + RandomUnderSampler
- `extract_features` - SelecciÃ³n de features con Mutual Information

### Estructura de MÃ³dulos

```
airflow/dags/
â”œâ”€â”€ etl_process_taskflow.py       # DAG principal con TaskFlow API
â””â”€â”€ etl_helpers/
    â”œâ”€â”€ __init__.py               # Package initialization
    â”œâ”€â”€ minio_utils.py            # Operaciones MinIO/S3
    â”œâ”€â”€ data_loader.py            # Descarga desde Socrata API
    â””â”€â”€ data_enrichment.py        # Enriquecimiento geoespacial y temporal
```

### ConfiguraciÃ³n

Las variables de entorno requeridas (`SOCRATA_APP_TOKEN`, `DATA_REPO_BUCKET_NAME`) estÃ¡n documentadas en la secciÃ³n [Variables de entorno requeridas](#variables-de-entorno-requeridas) al inicio de este documento.

**Dependencias principales:**
- `sodapy` - Cliente Socrata API
- `geopandas` - CÃ¡lculos geoespaciales
- `shapely` - GeometrÃ­as
- `pandas` - ManipulaciÃ³n de datos

### EjecuciÃ³n

**Trigger manual:**
1. Abrir Airflow UI: `make airflow`
2. Localizar DAG: `etl_with_taskflow`
3. Click en "Play" para ejecutar

**EjecuciÃ³n automÃ¡tica:**
- Schedule: `@monthly` (primer dÃ­a de cada mes)
- Primera ejecuciÃ³n: descarga aÃ±o completo
- Subsecuentes: solo Ãºltimo mes

### Monitoreo

**Ver logs:**
```bash
# Levantar CLI
docker compose --profile all --profile debug up

# Ejemplos de uso
docker-compose run airflow-cli config list              # Ver configuraciÃ³n
docker-compose run airflow-cli dags list                # Listar DAGs
docker-compose run airflow-cli tasks list etl_with_taskflow  # Listar tasks
docker-compose run airflow-cli dags trigger etl_with_taskflow  # Trigger manual
```

---

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Variables de Entorno (.env)

```bash
# Airflow
AIRFLOW_UID=50000                    # UID del usuario (Linux/Mac)
AIRFLOW_IMAGE_NAME=extending_airflow:latest

# PostgreSQL
PG_USER=airflow
PG_PASSWORD=airflow
MINIO_ACCESS_KEY=minio
MINIO_SECRET_ACCESS_KEY=minio123
MLFLOW_PORT=5001
FASTAPI_PORT=8800
```

### ETL Configuration

See `airflow/dags/etl_helpers/config.py`:

| Variable | Default | Description |
|----------|---------|-------------|
| `SPLIT_TEST_SIZE` | 0.2 | Test set proportion |
| `OUTLIER_STD_THRESHOLD` | 3 | Standard deviations for outlier removal |
| `MI_THRESHOLD` | 0.05 | Mutual Information threshold for feature selection |
| `SMOTE_SAMPLING_STRATEGY` | 0.5 | SMOTE oversampling ratio |

---

## ğŸ›  Commands

```bash
make help      # Show all commands
make up        # Start all services
make down      # Stop all services
make restart   # Restart all services
make install   # Rebuild containers
make clean     # Remove all containers and volumes
make logs      # Follow logs
make status    # Show service status
make lint      # Run linter
make format    # Format code
```

---

## ğŸ”§ Troubleshooting

### Port 5000 conflict (macOS)

AirPlay uses port 5000. MLflow is configured to use port 5001 by default.

### Permission denied on airflow/logs

```bash
chmod 777 airflow/logs
```

### Model not loading in API

```bash
# 1. Check if model is registered
curl http://localhost:8800/health

# 2. Reload model
curl -X POST http://localhost:8800/model/reload

# 3. Check MLflow for champion alias
docker-compose run trainer python -c "
import mlflow
mlflow.set_tracking_uri('http://mlflow:5000')
client = mlflow.tracking.MlflowClient()
model = client.get_registered_model('xgboost_chicago_crimes')
print('Aliases:', model.aliases)
"
```

### ETL fails at balance_data

This was fixed. Ensure NaN values are handled in `data_splitter.py`.

---

## ğŸ“ Project Structure

```
MLOPS-main/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ etl_process_taskflow.py    # Main ETL DAG
â”‚   â”‚   â””â”€â”€ etl_helpers/               # ETL modules
â”‚   â”‚       â”œâ”€â”€ config.py              # Centralized configuration
â”‚   â”‚       â”œâ”€â”€ data_loader.py         # Socrata API client
â”‚   â”‚       â”œâ”€â”€ data_enrichment.py     # Geospatial features
â”‚   â”‚       â”œâ”€â”€ data_splitter.py       # Train/test split
â”‚   â”‚       â”œâ”€â”€ data_encoding.py       # Feature encoding
â”‚   â”‚       â”œâ”€â”€ data_scaling.py        # Normalization
â”‚   â”‚       â”œâ”€â”€ data_balancing.py      # SMOTE balancing
â”‚   â”‚       â”œâ”€â”€ feature_selection.py   # MI-based selection
â”‚   â”‚       â”œâ”€â”€ monitoring.py          # MLflow logging
â”‚   â”‚       â””â”€â”€ minio_utils.py         # S3 operations
â”‚   â””â”€â”€ secrets/                        # Airflow secrets
â”œâ”€â”€ dockerfiles/
â”‚   â”œâ”€â”€ airflow/                        # Airflow image
â”‚   â”œâ”€â”€ fastapi/                        # API image
â”‚   â”œâ”€â”€ mlflow/                         # MLflow image
â”‚   â”œâ”€â”€ postgres/                       # PostgreSQL image
â”‚   â””â”€â”€ trainer/                        # Training image
â”œâ”€â”€ mlflow_scripts/
â”‚   â”œâ”€â”€ mlflow_xgboost_poc_docker.py   # XGBoost training script
â”‚   â”œâ”€â”€ champion_challenger.py          # Model promotion
â”‚   â””â”€â”€ predictor.py                    # Prediction utilities
â”œâ”€â”€ docker-compose.yaml                 # Service definitions
â”œâ”€â”€ Makefile                            # Command shortcuts
â””â”€â”€ README.md                           # This file
```

---

## ğŸ“„ License

Apache License 2.0

---

## ğŸ¤ Contributing

Pull requests are welcome. For major changes, please open an issue first.
