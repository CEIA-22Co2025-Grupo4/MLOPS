# Proyecto Final â€“ Operaciones de aprendizaje automÃ¡tico I
### ImplementaciÃ³n en ambiente productivo de un modelo de ML para la PredicciÃ³n de Arrestos en CrÃ­menes Reportados en la Ciudad de Chicago

## DescripciÃ³n General

Este proyecto implementa un **pipeline de Machine Learning end-to-end** orientado a la predicciÃ³n de la probabilidad de que **un delito registrado en la ciudad de Chicago** derive en un arresto.
El modelo utiliza un conjunto de variables diseÃ±adas para capturar informaciÃ³n clave del evento delictivo, entre ellas:

* **CaracterÃ­sticas del crimen**: cÃ³digo IUCR, categorÃ­a primaria, clasificaciÃ³n del FBI y tipo de delito.
* **InformaciÃ³n geoespacial**: coordenadas del incidente, distrito policial, Ã¡rea comunitaria y otros atributos territoriales.
* **Contexto temporal**: fecha y hora del hecho, dÃ­a de la semana, estaciÃ³n del aÃ±o y otras transformaciones temporales relevantes.
* **Proximidad operativa**: distancia al destacamento policial mÃ¡s cercano, incorporada como feature para capturar la influencia de la presencia policial.

## Objetivo del modelo

El objetivo del modelo es estimar la probabilidad de arresto asociado a un incidente delictivo en la ciudad de Chicago, utilizando un enfoque supervisado de clasificaciÃ³n.
El sistema toma como entrada los registros histÃ³ricos del Chicago Police Department y genera predicciones basadas en un conjunto de features que integran informaciÃ³n criminal, espacial, temporal y operativa (presencia policial).
Este modelo constituye el nÃºcleo del pipeline, sobre el cual se montan las tareas de entrenamiento, validaciÃ³n, seguimiento y despliegue

---

## ğŸ‘©â€ğŸ’» Autores

- **Daniel Eduardo PeÃ±aranda Peralta**
- **Jorge AdriÃ¡n Alvarez**
- **MarÃ­a BelÃ©n Cattaneo**
- **NicolÃ¡s ValentÃ­n Ciarrapico**
- **Sabrina Daiana Pryszczuk**

---

## ğŸ“‹ Tabla de Contenidos

1. [Arquitectura del Sistema](#-arquitectura-del-sistema)
2. [InstalaciÃ³n](#-instalaciÃ³n)
3. [Flujo de Trabajo Completo](#-flujo-de-trabajo-completo)
4. [ETL Pipeline](#-etl-pipeline)
5. [ExperimentaciÃ³n y Entrenamiento](#-experimentaciÃ³n-y-entrenamiento)
6. [Despliegue del Modelo](#-despliegue-del-modelo)
7. [API de PredicciÃ³n](#-api-de-predicciÃ³n)
8. [Monitoreo y MLflow](#-monitoreo-y-mlflow)
9. [Comandos Ãštiles](#-comandos-Ãºtiles)
10. [ConfiguraciÃ³n Avanzada](#-configuraciÃ³n-avanzada)

---

## ğŸ—ï¸ Arquitectura del Sistema

Este proyecto implementa un pipeline MLOps completo con los siguientes servicios:

- **[Apache Airflow](https://airflow.apache.org/)** - OrquestaciÃ³n de ETL y reentrenamiento
- **[MLflow](https://mlflow.org/)** - Tracking de experimentos y registro de modelos
- **[FastAPI](https://fastapi.tiangolo.com/)** - API REST para servir predicciones
- **[MinIO](https://min.io/)** - Almacenamiento de objetos S3-compatible
- **[PostgreSQL](https://www.postgresql.org/)** - Base de datos relacional
- **[ValKey](https://valkey.io/)** - Base de datos key-value (Redis fork)

![Diagrama de servicios](final_assign.png)

### Recursos Creados AutomÃ¡ticamente

**Buckets MinIO:**
- `s3://data` - Almacenamiento de datos del pipeline ETL
- `s3://mlflow` - Artefactos de experimentos y modelos

**Bases de Datos PostgreSQL:**
- `mlflow_db` - Metadata de MLflow
- `airflow` - Metadata de Airflow

---

## ğŸš€ InstalaciÃ³n

### Requisitos Previos

- [Docker](https://docs.docker.com/engine/install/) instalado
- Al menos 8GB RAM disponible
- 10GB espacio en disco

### Pasos de InstalaciÃ³n

1. **Clonar el repositorio:**
   ```bash
   git clone <repository-url>
   cd MLOPS
   ```

2. **Configurar permisos (Linux/MacOS):**
   ```bash
   # Crear carpetas necesarias
   mkdir -p airflow/{config,dags,logs,plugins}

   # Configurar UID en .env (encuentra tu UID con: id -u)
   echo "AIRFLOW_UID=$(id -u)" >> .env
   ```

3. **Configurar variables de entorno:**

   Edita el archivo `.env` y aÃ±ade tu token de Socrata API:
   ```bash
   SOCRATA_APP_TOKEN=tu_token_aqui
   ```

   ObtÃ©n tu token gratis en: https://data.cityofchicago.org/

4. **Levantar servicios:**
   ```bash
   make install && make up
   ```

   O usando docker-compose directamente:
   ```bash
   docker compose --profile all up
   ```

5. **Verificar estado:**
   ```bash
   docker ps -a  # Todos los servicios deben estar "healthy"
   ```

6. **Acceder a las interfaces:**
   - **Airflow UI:** http://localhost:8080 (user: `airflow`, pass: `airflow`)
   - **MLflow UI:** http://localhost:5001
   - **MinIO Console:** http://localhost:9001 (user: `minio`, pass: `minio123`)
   - **API Docs:** http://localhost:8800/docs
   - **API:** http://localhost:8800

> **Nota:** Si usas un servidor remoto, reemplaza `localhost` por la IP del servidor.

---

## ğŸ”„ Flujo de Trabajo Completo

Este proyecto sigue un flujo MLOps end-to-end:

<details>
  <summary><strong>ğŸ”» Clic aquÃ­ para ver el Diagrama de Flujo VersiÃ³n Texto (ASCII)</strong></summary>

  ```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     1. ETL PIPELINE (Airflow)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Download â”‚ â†’ â”‚ Enrich   â”‚ â†’ â”‚ Process  â”‚ â†’ â”‚ ML-Ready â”‚    â”‚
â”‚  â”‚   Data   â”‚   â”‚   Data   â”‚   â”‚   Data   â”‚   â”‚   Data   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚       â†“              â†“              â†“              â†“            â”‚
â”‚  [Raw Data]   [Enriched Data] [Processed]  [Train/Test]        â”‚
â”‚   MinIO s3://data/                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              2. EXPERIMENTACIÃ“N (Notebooks/Scripts)             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Experiment 1 â”‚   â”‚ Experiment 2 â”‚   â”‚ Experiment N â”‚       â”‚
â”‚  â”‚ (Logistic    â”‚   â”‚ (Random      â”‚   â”‚ (XGBoost)    â”‚       â”‚
â”‚  â”‚  Regression) â”‚   â”‚  Forest)     â”‚   â”‚              â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚         â†“                  â†“                  â†“                 â”‚
â”‚     MLflow Tracking UI - ComparaciÃ³n de mÃ©tricas               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Metrics: Accuracy, Precision, Recall, F1, AUC       â”‚       â”‚
â”‚  â”‚ Params: Hyperparameters, Features, Data version     â”‚       â”‚
â”‚  â”‚ Artifacts: Model, Charts, Feature importance        â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                3. REGISTRO DE MODELO (MLflow)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Seleccionar mejor modelo â†’ Register â†’ Production   â”‚        â”‚
â”‚  â”‚ Model Registry: Versioning, Staging, Production    â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  4. DESPLIEGUE (FastAPI)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  API carga modelo desde MLflow Model Registry    â”‚          â”‚
â”‚  â”‚  Endpoints:                                       â”‚          â”‚
â”‚  â”‚  - POST /predict - PredicciÃ³n individual         â”‚          â”‚
â”‚  â”‚  - POST /predict/batch - PredicciÃ³n por lote     â”‚          â”‚
â”‚  â”‚  - GET /model/info - Info del modelo en uso      â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              5. MONITOREO Y REENTRENAMIENTO                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  - Data drift monitoring                         â”‚          â”‚
â”‚  â”‚  - Model performance tracking                    â”‚          â”‚
â”‚  â”‚  - Automated retraining (Airflow DAG)            â”‚          â”‚
â”‚  â”‚  - A/B testing (Champion/Challenger)             â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
</details>

```mermaid 
graph LR
    %% --- ESTILOS ---
    %% Estilo para las cajas de contenido (alineadas a la izquierda para listas)
    classDef content fill:#0d1117,stroke:#30363d,stroke-width:1px,color:#c9d1d9,text-align:left,rx:5,ry:5;
    %% Estilo para los tÃ­tulos de las etapas
    classDef title fill:#161b22,stroke:#1f6feb,stroke-width:2px,color:#58a6ff,font-weight:bold;

    %% --- 1. ETL ---
    subgraph S1 ["1. ETL PIPELINE (Airflow)"]
        direction TB
        %% Definimos el flujo interno
        N1("ğŸ“¥ Download Data &rarr; Enrich Data<br>âš™ï¸ Process Data&rarr; ML-Ready Data"):::title
        D1("ğŸ’¾ MinIO s3://data/<br>------------------<br>â€¢ [Raw Data]<br>â€¢ [Enriched Data]<br>â€¢ [Processed]<br>â€¢ [Train/Test]"):::content
        N1 --> D1
    end

    %% --- 2. EXPERIMENTACIÃ“N ---
    subgraph S2 ["2. EXPERIMENTACIÃ“N"]
        direction TB
        N2("ğŸ§ª Notebooks/Scripts"):::title
        D2("ğŸ¤– Modelos:<br>â€¢ Exp. 1: Logistic Regression<br>â€¢ Exp. 2: Random Forest<br>â€¢ Exp. 3: XGBoost<br><br>ğŸ“Š MLflow Tracking UI:<br>â€¢ Metrics: Accuracy, Precision, Recall, F1, AUC<br>â€¢ Params: Hyperparameters, Features, Data version<br>â€¢ Artifacts: Model, Charts, Feature importance"):::content
        N2 --> D2
    end

    %% --- 3. REGISTRO DE MODELO---
    subgraph S3 ["3. REGISTRO (MLflow)"]
        direction TB
        N3("ğŸ¥‡ Model Registry"):::title
        D3("ğŸ“‹ Flujo:<br>1. Seleccionar Mejor<br>2. Registrar VersiÃ³n<br>3. Stage: 'Staging'<br>4. Stage: 'Production'"):::content
        N3 --> D3
    end

    %% --- 4. DESPLIEGUE ---
    subgraph S4 ["4. DESPLIEGUE (FastAPI)"]
        direction TB
        N4("ğŸš€ API REST"):::title
        D4("ğŸ”Œ Endpoints:<br>------------------<br>â€¢ POST /predict<br>  (PredicciÃ³n individual)<br>â€¢ POST /predict/batch<br>  (PredicciÃ³n por lote)<br>â€¢ GET /model/info (Info del modelo en uso)"):::content
        N4 --> D4
    end

    %% --- 5. MONITOREO ---
    subgraph S5 ["5. MANTENIMIENTO"]
        direction TB
        N5("ğŸ›¡ï¸ Monitoreo"):::title
        D5("âš ï¸ Data Drift check<br>ğŸ“ˆ Performance tracking<br>ğŸ”„ Automated Retraining (AirFlow DAG)<br>ğŸ†š A/B Testing (Champion/Challenger)"):::content
        N5 --> D5
    end

    %% --- CONEXIONES ---
    S1 --> S2
    S2 --> S3
    S3 --> S4
    S4 --> S5
    
    %% --- RETROALIMENTACION ---
    D5 -.-> |Trigger New Run| S1
```
---

## ğŸ”§ ETL Pipeline

### DescripciÃ³n General

Pipeline ETL automatizado que procesa datos de crÃ­menes de Chicago desde la API pÃºblica hasta datasets ML-ready.

### Arquitectura del Pipeline
<details>
  <summary><strong>ğŸ”» Clic aquÃ­ para ver el Diagrama de Flujo VersiÃ³n Texto (ASCII)</strong></summary>

  ```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Setup     â”‚ â†’ â”‚  Download   â”‚ â†’ â”‚   Enrich    â”‚ â†’ â”‚    Split    â”‚
â”‚     S3      â”‚   â”‚    Data     â”‚   â”‚    Data     â”‚   â”‚    Data     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Extract   â”‚ â† â”‚   Balance   â”‚ â† â”‚    Scale    â”‚ â† â”‚   Encode    â”‚
â”‚  Features   â”‚   â”‚    Data     â”‚   â”‚    Data     â”‚   â”‚    Data     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Pipeline  â”‚
â”‚   Summary   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
</details>

```mermaid
graph LR
    %% Cambiamos a LR (Left to Right) para que sea horizontal
    
    %% Estilos "Dark Mode Friendly" (Fondo oscuro, letra blanca)
    classDef steps fill:#1f425f,stroke:#82b1ff,stroke-width:2px,color:white,rx:5,ry:5;
    classDef startend fill:#1b4d3e,stroke:#4cc9f0,stroke-width:2px,color:white,rx:5,ry:5;

    %% Nodos
    A[Setup<br>S3]:::startend
    B[Download<br>Data]:::steps
    C[Enrich<br>Data]:::steps
    D[Split<br>Data]:::steps
    E[Encode<br>Data]:::steps
    F[Scale<br>Data]:::steps
    G[Balance<br>Data]:::steps
    H[Extract<br>Features]:::steps
    I[Pipeline<br>Summary]:::startend

    %% Conexiones
    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    F --> G
    G --> H
    H --> I
```

### Etapas del Pipeline

#### 1ï¸âƒ£ Setup S3
- Crea bucket MinIO si no existe
- Configura polÃ­tica de lifecycle (TTL 60 dÃ­as para datos temporales)

#### 2ï¸âƒ£ Download Data
- **Fuente:** [Chicago Data Portal](https://data.cityofchicago.org/) via Socrata API
- **Descarga inicial:** AÃ±o completo (~450k registros)
- **Descargas subsecuentes:** Incremental mensual (~35k registros)
- **Output:** `s3://data/0-raw-data/monthly-data/{YYYY-MM}/crimes.csv`

#### 3ï¸âƒ£ Enrich Data
- **Geoespacial:** Distancia a estaciÃ³n policial mÃ¡s cercana (GeoPandas)
- **Temporal:**
  - Season (Winter/Spring/Summer/Fall)
  - Day of week (0=Monday, 6=Sunday)
  - Day time (Morning/Afternoon/Evening/Night)
- **Limpieza:** Duplicados, valores nulos
- **Output:** `s3://data/1-enriched-data/crimes_enriched_{date}.csv`
- **Monitoring:** Logs raw data quality metrics to MLflow

#### 4ï¸âƒ£ Split Data
- **Estrategia:** Stratified train/test split (80/20)
- **Target:** `arrest` (boolean)
- **Output:**
  - `s3://data/2-split-data/crimes_train_{date}.csv`
  - `s3://data/2-split-data/crimes_test_{date}.csv`
- **Monitoring:** Logs class distribution to MLflow

#### 5ï¸âƒ£ Process Outliers
- **MÃ©todo:** IQR-based outlier removal
- **Log transformation:** `distance_crime_to_police_station`
- **Output:** `s3://data/3-outliers-data/`

#### 6ï¸âƒ£ Encode Data
- **One-hot:** Low cardinality (season, day_time)
- **Frequency:** High cardinality (primary_type, location_description)
- **Cyclic:** Day of week (sine transformation)
- **Label:** Boolean features (domestic)
- **Output:** `s3://data/4-encoded-data/`

#### 7ï¸âƒ£ Scale Data
- **MÃ©todo:** StandardScaler (zero mean, unit variance)
- **Features:** Numeric only (coordinates, distances)
- **Output:** `s3://data/5-scaled-data/`

#### 8ï¸âƒ£ Balance Data
- **Problema:** ~84% no arrest, ~16% arrest
- **SoluciÃ³n:** SMOTE + RandomUnderSampler
  - SMOTE: Oversample minority to 50% of majority
  - Undersampling: Final ratio 80% (minority = 80% of majority)
- **Output:** `s3://data/6-balanced-data/`
- **Monitoring:** Logs balancing impact to MLflow

#### 9ï¸âƒ£ Extract Features
- **MÃ©todo:** Mutual Information feature selection
- **Threshold:** MI score > 0.05
- **Features finales:** ~11 features (de ~20 originales)
- **Output:** `s3://data/ml-ready-data/train_{date}.csv`
- **Monitoring:** Logs feature importance and correlation to MLflow

#### ğŸ”Ÿ Pipeline Summary
- **ConsolidaciÃ³n:** MÃ©tricas de todo el pipeline
- **VisualizaciÃ³n:** Flow chart mostrando transformaciÃ³n de datos
- **Output:** MLflow run con pipeline overview

### Estructura de Datos en MinIO

```
s3://data/
â”œâ”€â”€ 0-raw-data/
â”‚   â””â”€â”€ monthly-data/
â”‚       â””â”€â”€ {YYYY-MM}/
â”‚           â”œâ”€â”€ crimes.csv              # ~35k registros/mes
â”‚           â””â”€â”€ police_stations.csv     # 23 estaciones
â”œâ”€â”€ 1-enriched-data/
â”‚   â””â”€â”€ crimes_enriched_{date}.csv      # +3 features temporales
â”œâ”€â”€ 2-split-data/
â”‚   â”œâ”€â”€ crimes_train_{date}.csv         # 80%
â”‚   â””â”€â”€ crimes_test_{date}.csv          # 20%
â”œâ”€â”€ 3-outliers-data/
â”‚   â”œâ”€â”€ train_{date}.csv
â”‚   â””â”€â”€ test_{date}.csv
â”œâ”€â”€ 4-encoded-data/
â”‚   â”œâ”€â”€ train_{date}.csv
â”‚   â””â”€â”€ test_{date}.csv
â”œâ”€â”€ 5-scaled-data/
â”‚   â”œâ”€â”€ train_{date}.csv
â”‚   â””â”€â”€ test_{date}.csv
â”œâ”€â”€ 6-balanced-data/
â”‚   â”œâ”€â”€ train_{date}.csv                # ~173k registros (balanced)
â”‚   â””â”€â”€ test_{date}.csv
â””â”€â”€ ml-ready-data/                      # â­ USAR ESTE PARA EXPERIMENTS
    â”œâ”€â”€ train_{date}.csv                # ~173k Ã— 11 features
    â””â”€â”€ test_{date}.csv                 # ~46k Ã— 11 features
```

### EjecuciÃ³n del Pipeline

**Trigger manual en Airflow UI:**
1. Navegar a http://localhost:8080
2. Buscar DAG: `etl_with_taskflow`
3. Click en â–¶ï¸ (Play) para ejecutar

**Schedule automÃ¡tico:**
- **Frecuencia:** `@monthly` (primer dÃ­a de cada mes a las 00:00)
- **Catchup:** Habilitado (procesa meses faltantes)
- **Max Active Runs:** 1 (evita ejecuciones concurrentes)

## Variables de entorno requeridas

Antes de ejecutar `make install`, asegÃºrate de configurar las siguientes variables en el archivo `.env`:

```bash
# Requeridas para el pipeline ETL
SOCRATA_APP_TOKEN=tu_token_aqui    # Token de Socrata API (ver instrucciones abajo)
DATA_REPO_BUCKET_NAME=data          # Bucket MinIO para datos

# Ya configuradas por defecto (modificar solo si es necesario)
AIRFLOW_UID=1000                    # UID del usuario para Airflow
AWS_ACCESS_KEY_ID=minio             # Credenciales MinIO
AWS_SECRET_ACCESS_KEY=minio123      # Credenciales MinIO
```

### Obtener Token de Socrata (Chicago Data Portal)

El pipeline ETL descarga datos del Chicago Data Portal usando la API de Socrata. Para evitar lÃ­mites de tasa, necesitas un App Token:

1. Ir a https://data.cityofchicago.org/
2. Crear una cuenta o iniciar sesiÃ³n (click en "Sign In" arriba a la derecha)
3. Una vez logueado, ir a tu perfil (click en tu nombre) â†’ "Developer Settings"
4. Click en "Create New App Token"
5. Completar el formulario:
   - **Application Name**: Nombre descriptivo (ej: "MLOps CEIA")
   - **Description**: DescripciÃ³n breve
   - **Website** (opcional): Puede dejarse vacÃ­o
6. Click en "Save" y copiar el **App Token** generado
7. Agregar el token al archivo `.env`:
   ```bash
   SOCRATA_APP_TOKEN=tu_token_generado_aqui
   ```

> **Nota**: Sin el token, el pipeline funcionarÃ¡ pero con lÃ­mites de velocidad mÃ¡s restrictivos.

## Comandos disponibles (Makefile)
### Monitoreo del Pipeline

Cada etapa del pipeline registra mÃ©tricas en **MLflow**:

**Runs creados automÃ¡ticamente:**
- `raw_data_{date}` - Calidad de datos crudos
- `split_{date}` - DistribuciÃ³n train/test
- `balance_{date}` - Impacto del balanceo
- `features_{date}` - Feature selection results
- `pipeline_summary_{date}` - Overview completo

**Artifacts en MLflow:**
- `charts/raw_data_overview.png` - 4 grÃ¡ficos de datos crudos
- `charts/split_distribution.png` - ComparaciÃ³n train/test
- `charts/balance_comparison.png` - Antes/despuÃ©s balanceo
- `charts/feature_importance.png` - Top 10 features (MI score)
- `charts/correlation_heatmap.png` - CorrelaciÃ³n entre features
- `charts/pipeline_flow.png` - â­ Data flow completo

---

## ğŸ§ª ExperimentaciÃ³n y Entrenamiento

### Acceso a Datos ML-Ready

Los datos procesados estÃ¡n disponibles en MinIO para tus experimentos:

```python
import os
import pandas as pd
import boto3

# Configurar conexiÃ³n a MinIO
os.environ["AWS_ACCESS_KEY_ID"] = "minio"
os.environ["AWS_SECRET_ACCESS_KEY"] = "minio123"
os.environ["AWS_ENDPOINT_URL_S3"] = "http://localhost:9000"

# Descargar datos
s3 = boto3.client('s3', endpoint_url='http://localhost:9000')

# Usar la Ãºltima versiÃ³n disponible (o especificar fecha)
train_df = pd.read_csv('s3://data/ml-ready-data/train_2025-11-22.csv')
test_df = pd.read_csv('s3://data/ml-ready-data/test_2025-11-22.csv')

# Separar features y target
X_train = train_df.drop('arrest', axis=1)
y_train = train_df['arrest']
X_test = test_df.drop('arrest', axis=1)
y_test = test_df['arrest']
```

### Template de ExperimentaciÃ³n

Ejemplo de experimento con tracking en MLflow:

```python
import mlflow
import mlflow.sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Configurar MLflow
mlflow.set_tracking_uri("http://localhost:5001")
mlflow.set_experiment("chicago-crime-arrest-prediction")

# Iniciar run
with mlflow.start_run(run_name="logistic_regression_baseline"):

    # Log parameters
    params = {
        "model_type": "LogisticRegression",
        "solver": "lbfgs",
        "max_iter": 1000,
        "class_weight": "balanced",
        "data_version": "2025-11-22"
    }
    mlflow.log_params(params)

    # Entrenar modelo
    model = LogisticRegression(**{k: v for k, v in params.items()
                                   if k not in ['model_type', 'data_version']})
    model.fit(X_train, y_train)

    # Predecir
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]

    # Log metrics
    metrics = {
        "accuracy": accuracy_score(y_test, y_pred),
        "precision": precision_score(y_test, y_pred),
        "recall": recall_score(y_test, y_pred),
        "f1": f1_score(y_test, y_pred),
        "roc_auc": roc_auc_score(y_test, y_pred_proba)
    }
    mlflow.log_metrics(metrics)

    # Log model
    mlflow.sklearn.log_model(
        model,
        "model",
        registered_model_name="chicago-crime-arrest-predictor"
    )

    print(f"âœ… Experiment logged to MLflow: {mlflow.active_run().info.run_id}")
```

### ComparaciÃ³n de Experimentos

Accede a MLflow UI para comparar experimentos:

1. **Navegar a:** http://localhost:5001
2. **Seleccionar experimento:** `chicago-crime-arrest-prediction`
3. **Comparar runs:** Seleccionar mÃºltiples runs y click en "Compare"
4. **Visualizar:**
   - Parallel coordinates plot
   - Scatter plot (metric vs metric)
   - Metric history
   - Artifact comparison

### Modelos Sugeridos para Experimentar

| Modelo | Fortalezas | HiperparÃ¡metros clave |
|--------|------------|----------------------|
| Logistic Regression | Baseline rÃ¡pido, interpretable | `C`, `solver`, `class_weight` |
| Random Forest | Robusto, feature importance | `n_estimators`, `max_depth`, `min_samples_split` |
| XGBoost | Alto rendimiento, manejo de desbalance | `learning_rate`, `max_depth`, `scale_pos_weight` |
| LightGBM | RÃ¡pido, eficiente en memoria | `num_leaves`, `learning_rate`, `feature_fraction` |
| CatBoost | Manejo automÃ¡tico de categÃ³ricas | `iterations`, `learning_rate`, `depth` |

---

## ğŸš€ Despliegue del Modelo

### Registro del Modelo en MLflow

1. **Entrenar y loguear modelo** (ver secciÃ³n ExperimentaciÃ³n)

2. **Registrar modelo en Model Registry:**
   ```python
   # OpciÃ³n 1: Durante el training
   mlflow.sklearn.log_model(
       model,
       "model",
       registered_model_name="chicago-crime-arrest-predictor"
   )

   # OpciÃ³n 2: Desde run existente
   run_id = "abc123..."
   model_uri = f"runs:/{run_id}/model"
   mlflow.register_model(model_uri, "chicago-crime-arrest-predictor")
   ```

3. **Promover a Production:**
   ```python
   from mlflow.tracking import MlflowClient

   client = MlflowClient()

   # Obtener Ãºltima versiÃ³n
   model_name = "chicago-crime-arrest-predictor"
   latest_version = client.get_latest_versions(model_name, stages=["None"])[0]

   # Promover a Production
   client.transition_model_version_stage(
       name=model_name,
       version=latest_version.version,
       stage="Production"
   )
   ```

### FastAPI - Carga del Modelo

La API carga automÃ¡ticamente el modelo en stage "Production" desde MLflow:

```python
# En dockerfiles/fastapi/app/main.py
import mlflow.pyfunc

MODEL_NAME = "chicago-crime-arrest-predictor"
model = mlflow.pyfunc.load_model(f"models:/{MODEL_NAME}/Production")
```

---

## ğŸŒ API de PredicciÃ³n

### Endpoints Disponibles

#### 1. PredicciÃ³n Individual

```bash
POST /predict
```

**Request:**
```json
{
  "primary_type_freq": 0.123,
  "location_description_freq": 0.045,
  "beat_freq": 0.012,
  "ward_freq": 0.034,
  "community_area_freq": 0.028,
  "day_of_week_sin": 0.781,
  "x_coordinate_standardized": 1.234,
  "longitude_standardized": -0.567,
  "latitude_standardized": 0.890,
  "y_coordinate_standardized": -1.123,
  "distance_crime_to_police_station_standardized": 0.345
}
```

**Response:**
```json
{
  "prediction": true,
  "probability": 0.78,
  "model_version": "2",
  "timestamp": "2025-11-22T10:30:00Z"
}
```

#### 2. PredicciÃ³n por Lote

```bash
POST /predict/batch
```

**Request:**
```json
{
  "instances": [
    { "primary_type_freq": 0.123, ... },
    { "primary_type_freq": 0.456, ... }
  ]
}
```

**Response:**
```json
{
  "predictions": [
    {
      "prediction": true,
      "probability": 0.78
    },
    {
      "prediction": false,
      "probability": 0.23
    }
  ],
  "model_version": "2",
  "timestamp": "2025-11-22T10:30:00Z"
}
```

#### 3. InformaciÃ³n del Modelo

```bash
GET /model/info
```

**Response:**
```json
{
  "name": "chicago-crime-arrest-predictor",
  "version": "2",
  "stage": "Production",
  "description": "XGBoost classifier for arrest prediction",
  "metrics": {
    "accuracy": 0.85,
    "precision": 0.82,
    "recall": 0.79,
    "f1": 0.80,
    "roc_auc": 0.91
  }
}
```

### Ejemplos de Uso

**cURL:**
```bash
curl -X POST "http://localhost:8800/predict" \
  -H "Content-Type: application/json" \
  -d '{
    "primary_type_freq": 0.123,
    "location_description_freq": 0.045,
    "beat_freq": 0.012,
    "ward_freq": 0.034,
    "community_area_freq": 0.028,
    "day_of_week_sin": 0.781,
    "x_coordinate_standardized": 1.234,
    "longitude_standardized": -0.567,
    "latitude_standardized": 0.890,
    "y_coordinate_standardized": -1.123,
    "distance_crime_to_police_station_standardized": 0.345
  }'
```

**Python:**
```python
import requests

url = "http://localhost:8800/predict"
data = {
    "primary_type_freq": 0.123,
    "location_description_freq": 0.045,
    # ... resto de features
}

response = requests.post(url, json=data)
print(response.json())
```

**DocumentaciÃ³n Interactiva:**
- Swagger UI: http://localhost:8800/docs
- ReDoc: http://localhost:8800/redoc

---

## ğŸ“Š Monitoreo y MLflow

### Acceso a MLflow UI

```bash
# Abrir en navegador
open http://localhost:5001

# O usar make command
make mlflow
```

### Experiments Creados

| Experiment | DescripciÃ³n | Runs |
|------------|-------------|------|
| `Default` | Runs del pipeline ETL | `raw_data_*`, `split_*`, `balance_*`, `features_*`, `pipeline_summary_*` |
| `chicago-crime-arrest-prediction` | Experimentos de modelos | Tus experiments de entrenamiento |

### MÃ©tricas del Pipeline (ETL)

Cada ejecuciÃ³n del pipeline crea 5 runs en MLflow:

**1. `raw_data_{date}`**
- MÃ©tricas: total_records, arrest_rate_pct, unique_districts, etc.
- Artifacts: `charts/raw_data_overview.png`

**2. `split_{date}`**
- MÃ©tricas: train_size, test_size, class distribution
- Artifacts: `charts/split_distribution.png`

**3. `balance_{date}`**
- MÃ©tricas: original_size, balanced_size, class_ratio improvement
- Artifacts: `charts/balance_comparison.png`

**4. `features_{date}`**
- MÃ©tricas: selected_features, dropped_features, feature_reduction_pct
- Artifacts: `charts/feature_importance.png`, `charts/correlation_heatmap.png`

**5. `pipeline_summary_{date}` â­**
- MÃ©tricas: Todas las counts + retention percentages
- Artifacts: `charts/pipeline_flow.png` (overview completo del pipeline)

### Model Registry

**Estados del Modelo:**
- `None` - ReciÃ©n registrado
- `Staging` - En pruebas
- `Production` - Desplegado en API
- `Archived` - VersiÃ³n antigua

**Transiciones:**
```python
from mlflow.tracking import MlflowClient

client = MlflowClient()

# Staging â†’ Production
client.transition_model_version_stage(
    name="chicago-crime-arrest-predictor",
    version="3",
    stage="Production"
)

# Archivar versiÃ³n antigua
client.transition_model_version_stage(
    name="chicago-crime-arrest-predictor",
    version="2",
    stage="Archived"
)
```

---

## ğŸ› ï¸ Comandos Ãštiles

### Makefile Commands

```bash
make help      # Muestra todos los comandos disponibles
make up        # Inicia todos los servicios
make down      # Detiene todos los servicios
make restart   # Reinicia todos los servicios
make install   # Reconstruye contenedores con nuevas dependencias
make clean     # Detiene y elimina todo (âš ï¸ borra datos)
make logs      # Muestra logs de todos los servicios
make status    # Estado de todos los servicios
```

### Flujos de Trabajo Comunes

**Primera vez:**
```bash
make install && make up
```

**Agregar dependencias:**
```bash
# 1. Editar requirements.txt en dockerfiles/airflow/ o dockerfiles/fastapi/
# 2. Reconstruir
make install
```

**Reiniciar desde cero:**
```bash
make clean
make install && make up
```

## ETL Pipeline: Chicago Crime Data

El proyecto incluye un pipeline ETL completo para anÃ¡lisis de crÃ­menes en Chicago:

### Arquitectura del Pipeline

**Task 1: setup_s3**
- Crea bucket MinIO si no existe
- Configura polÃ­tica de lifecycle (TTL de 60 dÃ­as para datos temporales)

**Task 2: download_data**
- Descarga datos de crÃ­menes desde Socrata API (Chicago Data Portal)
- Descarga datos de estaciones policiales
- Primera ejecuciÃ³n: descarga aÃ±o completo (~260k registros)
- Ejecuciones subsecuentes: descarga incremental mensual (~25k registros)
- Guarda en MinIO: `raw-data/crimes_YYYY-MM-DD.csv` y `raw-data/police_stations.csv`

**Task 3: enrich_data**
- Carga datos desde MinIO
- Calcula distancia a estaciÃ³n policial mÃ¡s cercana (GeoPandas spatial join)
- Crea features temporales:
  - Season (Winter/Spring/Summer/Fall)
  - Day of week (0-6)
  - Day time (Morning/Afternoon/Evening/Night)
- Guarda en MinIO: `raw-data/crimes_enriched_YYYY-MM-DD.csv`

**Tasks 4-8:** (Por implementar)
- `split_data` - DivisiÃ³n train/test (80/20 estratificado)
- `process_outliers` - Manejo de outliers y transformaciÃ³n logarÃ­tmica
- `encode_data` - Encoding de variables categÃ³ricas
- `scale_data` - Escalado con StandardScaler
- `balance_data` - Balanceo con SMOTE + RandomUnderSampler
- `extract_features` - SelecciÃ³n de features con Mutual Information

### Estructura de MÃ³dulos

```
airflow/dags/
â”œâ”€â”€ etl_process_taskflow.py       # DAG principal con TaskFlow API
â””â”€â”€ etl_helpers/
    â”œâ”€â”€ __init__.py               # Package initialization
    â”œâ”€â”€ minio_utils.py            # Operaciones MinIO/S3
    â”œâ”€â”€ data_loader.py            # Descarga desde Socrata API
    â””â”€â”€ data_enrichment.py        # Enriquecimiento geoespacial y temporal
```

### ConfiguraciÃ³n

Las variables de entorno requeridas (`SOCRATA_APP_TOKEN`, `DATA_REPO_BUCKET_NAME`) estÃ¡n documentadas en la secciÃ³n [Variables de entorno requeridas](#variables-de-entorno-requeridas) al inicio de este documento.

**Dependencias principales:**
- `sodapy` - Cliente Socrata API
- `geopandas` - CÃ¡lculos geoespaciales
- `shapely` - GeometrÃ­as
- `pandas` - ManipulaciÃ³n de datos

### EjecuciÃ³n

**Trigger manual:**
1. Abrir Airflow UI: `make airflow`
2. Localizar DAG: `etl_with_taskflow`
3. Click en "Play" para ejecutar

**EjecuciÃ³n automÃ¡tica:**
- Schedule: `@monthly` (primer dÃ­a de cada mes)
- Primera ejecuciÃ³n: descarga aÃ±o completo
- Subsecuentes: solo Ãºltimo mes

### Monitoreo

**Ver logs:**
```bash
# Levantar CLI
docker compose --profile all --profile debug up

# Ejemplos de uso
docker-compose run airflow-cli config list              # Ver configuraciÃ³n
docker-compose run airflow-cli dags list                # Listar DAGs
docker-compose run airflow-cli tasks list etl_with_taskflow  # Listar tasks
docker-compose run airflow-cli dags trigger etl_with_taskflow  # Trigger manual
```

---

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Variables de Entorno (.env)

```bash
# Airflow
AIRFLOW_UID=50000                    # UID del usuario (Linux/Mac)
AIRFLOW_IMAGE_NAME=extending_airflow:latest

# PostgreSQL
PG_USER=airflow
PG_PASSWORD=airflow
PG_DATABASE=airflow
PG_PORT=5432

# MinIO
MINIO_ACCESS_KEY=minio
MINIO_SECRET_ACCESS_KEY=minio123
MINIO_PORT=9000
MINIO_PORT_UI=9001

# MLflow
MLFLOW_PORT=5001
MLFLOW_BUCKET_NAME=mlflow

# Data
DATA_REPO_BUCKET_NAME=data
SOCRATA_APP_TOKEN=tu_token_aqui     # âš ï¸ REQUERIDO

# FastAPI
FASTAPI_PORT=8800
```

### ConexiÃ³n a MinIO desde Local

Para usar boto3, awscli, o pandas desde tu mÃ¡quina local:

```bash
make airflow   # Ver DAG runs y logs
make minio     # Ver archivos en buckets
```

### Datos de Salida

**UbicaciÃ³n:** MinIO bucket `data/`

**Estructura:**
```
data/
â”œâ”€â”€ 0-raw-data/
â”‚   â”œâ”€â”€ monthly-data/
â”‚   â”‚   â””â”€â”€ {YYYY-MM}/
â”‚   â”‚       â”œâ”€â”€ crimes.csv              # CrÃ­menes descargados del mes
â”‚   â”‚       â””â”€â”€ police_stations.csv     # Estaciones policiales
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ crimes_12m_{YYYY-MM-DD}.csv # Ventana rolling 12 meses
â”œâ”€â”€ 1-enriched-data/
â”‚   â””â”€â”€ crimes_enriched_{YYYY-MM-DD}.csv # CrÃ­menes enriquecidos
â””â”€â”€ 2-processed-data/                    # (PrÃ³ximos pasos)
    â”œâ”€â”€ train_encoded.csv
    â”œâ”€â”€ test_encoded.csv
    â””â”€â”€ ...
```

## Apagar los servicios

Estos servicios ocupan cierta cantidad de memoria RAM y procesamiento, por lo que cuando no se estÃ¡n utilizando, se recomienda detenerlos. Para hacerlo, ejecuta:

```bash
make down
```

**Eliminar todo (âš ï¸ borra datos):**
```bash
make clean
```

**Usando docker-compose directamente:**
```bash
# Solo detener
docker compose --profile all down

# Eliminar todo
docker compose down --rmi all --volumes
```

**Nota:** Si haces esto, perderÃ¡s todo en los buckets y bases de datos.

## Aspectos especÃ­ficos de Airflow

### Variables de entorno
Airflow ofrece una amplia gama de opciones de configuraciÃ³n. En el archivo `docker-compose.yaml`, dentro de `x-airflow-common`, se encuentran variables de entorno que pueden modificarse para ajustar la configuraciÃ³n de Airflow. Pueden aÃ±adirse [otras variables](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html).

### Uso de ejecutores externos
Actualmente, para este caso, Airflow utiliza un ejecutor [celery](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/executor/celery.html), lo que significa que las tareas se ejecutan en otro contenedor. 

### Uso de la CLI de Airflow

Si necesitan depurar Apache Airflow, pueden utilizar la CLI de Apache Airflow de la siguiente manera:

```bash
docker compose --profile all --profile debug up
```

Una vez que el contenedor estÃ© en funcionamiento, pueden utilizar la CLI de Airflow de la siguiente manera, 
por ejemplo, para ver la configuraciÃ³n:

```bash
docker-compose run airflow-cli config list      
```

Para obtener mÃ¡s informaciÃ³n sobre el comando, pueden consultar [aqui](https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html).

### Variables y Conexiones

Si desean agregar variables para accederlas en los DAGs, pueden hacerlo en `secrets/variables.yaml`. Para obtener mÃ¡s [informaciÃ³n](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/variables.html), 
consulten la documentaciÃ³n.

Si desean agregar conexiones en Airflow, pueden hacerlo en `secrets/connections.yaml`. TambiÃ©n es posible agregarlas mediante la interfaz de usuario (UI), pero estas no persistirÃ¡n si se borra todo. Por otro lado, cualquier conexiÃ³n guardada en `secrets/connections.yaml` no aparecerÃ¡ en la UI, aunque eso no significa que no exista. Consulten la documentaciÃ³n para obtener mÃ¡s 
[informaciÃ³n](https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/connections.html).

## ConexiÃ³n con los buckets

Dado que no estamos utilizando Amazon S3, sino una implementaciÃ³n local de los mismos mediante MinIO, es necesario modificar las variables de entorno para conectar con el servicio de MinIO. Las variables de entorno son las siguientes:

```bash
AWS_ACCESS_KEY_ID=minio   
AWS_SECRET_ACCESS_KEY=minio123 
AWS_ENDPOINT_URL_S3=http://localhost:90000
```

MLflow tambiÃ©n tiene una variable de entorno que afecta su conexiÃ³n a los buckets:

```bash
MLFLOW_S3_ENDPOINT_URL=http://localhost:9000
```
AsegÃºrate de establecer estas variables de entorno antes de ejecutar tu notebook o scripts en tu mÃ¡quina o en cualquier otro lugar. Si estÃ¡s utilizando un servidor externo a tu computadora de trabajo, reemplaza localhost por su direcciÃ³n IP.

Al hacer esto, podrÃ¡s utilizar `boto3`, `awswrangler`, etc., en Python con estos buckets, o `awscli` en la consola.

Si tienes acceso a AWS S3, ten mucho cuidado de no reemplazar tus credenciales de AWS. Si usas las variables de entorno, no tendrÃ¡s problemas.

## Valkey

La base de datos Valkey es usada por Apache Airflow para su funcionamiento. Tal como estÃ¡ configurado ahora no esta expuesto el puerto para poder ser usado externamente. Se puede modificar el archivo `docker-compose.yaml` para habilitaro.

## Pull Request

Este repositorio estÃ¡ abierto para que realicen sus propios Pull Requests y asÃ­ contribuir a mejorarlo. Si desean realizar alguna modificaciÃ³n, **Â¡son bienvenidos!** TambiÃ©n se pueden crear nuevos entornos productivos para aumentar la variedad de implementaciones, idealmente en diferentes `branches`. Algunas ideas que se me ocurren que podrÃ­an implementar son:

- Reemplazar Airflow y MLflow con [Metaflow](https://metaflow.org/) o [Kubeflow](https://www.kubeflow.org).
- Reemplazar MLflow con [Seldon-Core](https://github.com/SeldonIO/seldon-core).
- Agregar un servicio de tableros como, por ejemplo, [Grafana](https://grafana.com).
